{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "    <h1>Exercise 5.5</h1>\n",
    "    <p>In Chapter 4, we used logistic regression to predict the probability of $\\mathrm{default}$ using $\\mathrm{income}$ and $\\mathrm{balance}$ on the <code>Default</code> data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis.</p>\n",
    "    <ol>\n",
    "        <li>Fit a logistic regression model that uses $\\mathrm{income}$ and $\\mathrm{balance}$ to predict $\\mathrm{default}$.</li>\n",
    "        <li>Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:\n",
    "            <ol>\n",
    "                <li>Split the sample set into a training set and a validation set.</li>\n",
    "                <li>Fit a multiple logistic regression model using only the training observations.</li>\n",
    "                <li>Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the $\\mathrm{default}$ category if the posterior probability is greater than $0.5$.</li>\n",
    "                <li>Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.</li>\n",
    "            </ol>\n",
    "        </li>\n",
    "        <li>Repeat the process in 2 three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained.</li>\n",
    "        <li>Now consider a logistic regression model that predicts the probability of $\\mathrm{default}$ using $\\mathrm{income}$, $\\mathrm{balance}$, and a dummy variable for $\\mathrm{student}$. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for $\\mathrm{student}$ leads to a reduction in the test error rate.</li>\n",
    "    </ol>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%run ../../customModules/usefulFunctions.ipynb\n",
    "# https://stackoverflow.com/questions/34398054/ipython-notebook-cell-multiple-outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exercise 5.5.1</h3>\n",
    "<blockquote>\n",
    "    <i>Fit a logistic regression model that uses $\\mathrm{income}$ and $\\mathrm{balance}$ to predict $\\mathrm{default}$.</i>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078948\n",
      "         Iterations 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>default01</td>    <th>  No. Observations:  </th>   <td> 10000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  9997</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sun, 12 Jan 2020</td> <th>  Pseudo R-squ.:     </th>   <td>0.4594</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>20:51:57</td>     <th>  Log-Likelihood:    </th>  <td> -789.48</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -1460.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>4.541e-292</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>  -11.5405</td> <td>    0.435</td> <td>  -26.544</td> <td> 0.000</td> <td>  -12.393</td> <td>  -10.688</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>balance</th>   <td>    0.0056</td> <td>    0.000</td> <td>   24.835</td> <td> 0.000</td> <td>    0.005</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income</th>    <td> 2.081e-05</td> <td> 4.99e-06</td> <td>    4.174</td> <td> 0.000</td> <td>  1.1e-05</td> <td> 3.06e-05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.14 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              default01   No. Observations:                10000\n",
       "Model:                          Logit   Df Residuals:                     9997\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Sun, 12 Jan 2020   Pseudo R-squ.:                  0.4594\n",
       "Time:                        20:51:57   Log-Likelihood:                -789.48\n",
       "converged:                       True   LL-Null:                       -1460.3\n",
       "Covariance Type:            nonrobust   LLR p-value:                4.541e-292\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept    -11.5405      0.435    -26.544      0.000     -12.393     -10.688\n",
       "balance        0.0056      0.000     24.835      0.000       0.005       0.006\n",
       "income      2.081e-05   4.99e-06      4.174      0.000     1.1e-05    3.06e-05\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.14 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../DataSets/Default/Default.csv\")\n",
    "df['default01'] = np.where(df['default'] == 'Yes', 1, 0)\n",
    "df.insert(0, 'Intercept', 1)\n",
    "targetColumn = ['default01']\n",
    "descriptiveColumns = ['Intercept', 'balance', 'income']\n",
    "df_X = df[descriptiveColumns]\n",
    "df_Y = df[targetColumn]\n",
    "model = sm.Logit(df_Y, df_X)\n",
    "fitted = model.fit()\n",
    "fitted.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exercise 5.5.2</h3>\n",
    "<blockquote>\n",
    "    <i>Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:\n",
    "            <ol>\n",
    "                <li>Split the sample set into a training set and a validation set.</li>\n",
    "                <li>Fit a multiple logistic regression model using only the training observations.</li>\n",
    "                <li>Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the $\\mathrm{default}$ category if the posterior probability is greater than $0.5$.</li>\n",
    "                <li>Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.</li>\n",
    "            </ol></i>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078493\n",
      "         Iterations 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>default01</td>    <th>  No. Observations:  </th>   <td>  5000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  4997</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sun, 12 Jan 2020</td> <th>  Pseudo R-squ.:     </th>   <td>0.4804</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>20:51:58</td>     <th>  Log-Likelihood:    </th>  <td> -392.46</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -755.25</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>2.774e-158</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>  -11.9681</td> <td>    0.640</td> <td>  -18.688</td> <td> 0.000</td> <td>  -13.223</td> <td>  -10.713</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>balance</th>   <td>    0.0060</td> <td>    0.000</td> <td>   17.665</td> <td> 0.000</td> <td>    0.005</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income</th>    <td> 1.934e-05</td> <td> 6.99e-06</td> <td>    2.766</td> <td> 0.006</td> <td> 5.63e-06</td> <td>  3.3e-05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.18 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              default01   No. Observations:                 5000\n",
       "Model:                          Logit   Df Residuals:                     4997\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Sun, 12 Jan 2020   Pseudo R-squ.:                  0.4804\n",
       "Time:                        20:51:58   Log-Likelihood:                -392.46\n",
       "converged:                       True   LL-Null:                       -755.25\n",
       "Covariance Type:            nonrobust   LLR p-value:                2.774e-158\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept    -11.9681      0.640    -18.688      0.000     -13.223     -10.713\n",
       "balance        0.0060      0.000     17.665      0.000       0.005       0.007\n",
       "income      1.934e-05   6.99e-06      2.766      0.006    5.63e-06     3.3e-05\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.18 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>less than median</th>\n",
       "      <th>greater than median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Observed</th>\n",
       "      <th>less than median</th>\n",
       "      <td>4818</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>greater than median</th>\n",
       "      <td>106</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Predicted                    \n",
       "                             less than median greater than median\n",
       "Observed less than median                4818                  23\n",
       "         greater than median              106                  53"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Predicted (%)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>less than median</th>\n",
       "      <th>greater than median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Observed (%)</th>\n",
       "      <th>less than median</th>\n",
       "      <td>99.52</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>greater than median</th>\n",
       "      <td>66.67</td>\n",
       "      <td>33.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Predicted (%)                    \n",
       "                                 less than median greater than median\n",
       "Observed (%) less than median               99.52                0.48\n",
       "             greater than median            66.67               33.33"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'The validation set error is 2.58%.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.5, random_state=42)\n",
    "df_X_train = df_train[descriptiveColumns]\n",
    "df_Y_train = df_train[targetColumn]\n",
    "df_X_test = df_test[descriptiveColumns]\n",
    "df_Y_test = df_test[targetColumn]\n",
    "\n",
    "model = sm.Logit(df_Y_train, df_X_train)\n",
    "fitted = model.fit()\n",
    "fitted.summary()\n",
    "\n",
    "sr_Y_pred = fitted.predict(df_X_test)\n",
    "df_Y_test_and_pred = pd.DataFrame({\n",
    "    'Observed': df_Y_test['default01'],\n",
    "    'Predicted': np.where(sr_Y_pred > 0.5, 1, 0),\n",
    "})\n",
    "df_confusion, df_confusion_pct = createConfusionMatrixFromOutOfSampleData(df=df_Y_test_and_pred, binaryMap={0: 'less than median', 1: 'greater than median'})\n",
    "df_confusion  \n",
    "df_confusion_pct.round(2)\n",
    "\n",
    "confusion_matrix = df_confusion.to_numpy()\n",
    "TN, FP, FN, TP = confusion_matrix[0, 0], confusion_matrix[0, 1], confusion_matrix[1, 0], confusion_matrix[1, 1]\n",
    "missclass_acc = 100 * ((FP + FN) / (TN + FP + FN + TP))\n",
    "f'The validation set error is {missclass_acc:.2f}%.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exercise 5.5.3</h3>\n",
    "<blockquote>\n",
    "    <i>Repeat the process in 2 three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained.</i>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078493\n",
      "         Iterations 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The validation set error is 2.58%.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'-----------------------'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078493\n",
      "         Iterations 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The validation set error is 2.86%.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'-----------------------'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078493\n",
      "         Iterations 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The validation set error is 2.54%.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.5, random_state=42)\n",
    "df_X_train = df_train[descriptiveColumns]\n",
    "df_Y_train = df_train[targetColumn]\n",
    "df_X_test = df_test[descriptiveColumns]\n",
    "df_Y_test = df_test[targetColumn]\n",
    "\n",
    "model = sm.Logit(df_Y_train, df_X_train)\n",
    "fitted = model.fit()\n",
    "\n",
    "sr_Y_pred = fitted.predict(df_X_test)\n",
    "df_Y_test_and_pred = pd.DataFrame({\n",
    "    'Observed': df_Y_test['default01'],\n",
    "    'Predicted': np.where(sr_Y_pred > 0.5, 1, 0),\n",
    "})\n",
    "df_confusion, df_confusion_pct = createConfusionMatrixFromOutOfSampleData(df=df_Y_test_and_pred, binaryMap={0: 'less than median', 1: 'greater than median'})\n",
    "confusion_matrix = df_confusion.to_numpy()\n",
    "TN, FP, FN, TP = confusion_matrix[0, 0], confusion_matrix[0, 1], confusion_matrix[1, 0], confusion_matrix[1, 1]\n",
    "missclass_acc = 100 * ((FP + FN) / (TN + FP + FN + TP))\n",
    "f'The validation set error is {missclass_acc:.2f}%.'\n",
    "\n",
    "'-----------------------'\n",
    "\n",
    "f_train, df_test = train_test_split(df, test_size=0.5, random_state=43)\n",
    "df_X_train = df_train[descriptiveColumns]\n",
    "df_Y_train = df_train[targetColumn]\n",
    "df_X_test = df_test[descriptiveColumns]\n",
    "df_Y_test = df_test[targetColumn]\n",
    "\n",
    "model = sm.Logit(df_Y_train, df_X_train)\n",
    "fitted = model.fit()\n",
    "\n",
    "sr_Y_pred = fitted.predict(df_X_test)\n",
    "df_Y_test_and_pred = pd.DataFrame({\n",
    "    'Observed': df_Y_test['default01'],\n",
    "    'Predicted': np.where(sr_Y_pred > 0.5, 1, 0),\n",
    "})\n",
    "df_confusion, df_confusion_pct = createConfusionMatrixFromOutOfSampleData(df=df_Y_test_and_pred, binaryMap={0: 'less than median', 1: 'greater than median'})\n",
    "confusion_matrix = df_confusion.to_numpy()\n",
    "TN, FP, FN, TP = confusion_matrix[0, 0], confusion_matrix[0, 1], confusion_matrix[1, 0], confusion_matrix[1, 1]\n",
    "missclass_acc = 100 * ((FP + FN) / (TN + FP + FN + TP))\n",
    "f'The validation set error is {missclass_acc:.2f}%.'\n",
    "\n",
    "'-----------------------'\n",
    "\n",
    "f_train, df_test = train_test_split(df, test_size=0.5, random_state=44)\n",
    "df_X_train = df_train[descriptiveColumns]\n",
    "df_Y_train = df_train[targetColumn]\n",
    "df_X_test = df_test[descriptiveColumns]\n",
    "df_Y_test = df_test[targetColumn]\n",
    "\n",
    "model = sm.Logit(df_Y_train, df_X_train)\n",
    "fitted = model.fit()\n",
    "\n",
    "sr_Y_pred = fitted.predict(df_X_test)\n",
    "df_Y_test_and_pred = pd.DataFrame({\n",
    "    'Observed': df_Y_test['default01'],\n",
    "    'Predicted': np.where(sr_Y_pred > 0.5, 1, 0),\n",
    "})\n",
    "df_confusion, df_confusion_pct = createConfusionMatrixFromOutOfSampleData(df=df_Y_test_and_pred, binaryMap={0: 'less than median', 1: 'greater than median'})\n",
    "confusion_matrix = df_confusion.to_numpy()\n",
    "TN, FP, FN, TP = confusion_matrix[0, 0], confusion_matrix[0, 1], confusion_matrix[1, 0], confusion_matrix[1, 1]\n",
    "missclass_acc = 100 * ((FP + FN) / (TN + FP + FN + TP))\n",
    "f'The validation set error is {missclass_acc:.2f}%.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The average validation set error of these 3 fitted models is $2.66 \\%$.</p>\n",
    "\n",
    "<h3>Exercise 5.5.4</h3>\n",
    "<blockquote>\n",
    "    <i>Now consider a logistic regression model that predicts the probability of $\\mathrm{default}$ using $\\mathrm{income}$, $\\mathrm{balance}$, and a dummy variable for $\\mathrm{student}$. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for $\\mathrm{student}$ leads to a reduction in the test error rate.</i>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.077900\n",
      "         Iterations 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The validation set error is 2.56%.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'-----------------------'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.077900\n",
      "         Iterations 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The validation set error is 2.88%.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'-----------------------'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.077900\n",
      "         Iterations 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The validation set error is 2.56%.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['student01'] = np.where(df['student'] == 'Yes', 1, 0)\n",
    "descriptiveColumns = ['Intercept', 'balance', 'income', 'student01']\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.5, random_state=42)\n",
    "df_X_train = df_train[descriptiveColumns]\n",
    "df_Y_train = df_train[targetColumn]\n",
    "df_X_test = df_test[descriptiveColumns]\n",
    "df_Y_test = df_test[targetColumn]\n",
    "\n",
    "model = sm.Logit(df_Y_train, df_X_train)\n",
    "fitted = model.fit()\n",
    "\n",
    "sr_Y_pred = fitted.predict(df_X_test)\n",
    "df_Y_test_and_pred = pd.DataFrame({\n",
    "    'Observed': df_Y_test['default01'],\n",
    "    'Predicted': np.where(sr_Y_pred > 0.5, 1, 0),\n",
    "})\n",
    "df_confusion, df_confusion_pct = createConfusionMatrixFromOutOfSampleData(df=df_Y_test_and_pred, binaryMap={0: 'less than median', 1: 'greater than median'})\n",
    "confusion_matrix = df_confusion.to_numpy()\n",
    "TN, FP, FN, TP = confusion_matrix[0, 0], confusion_matrix[0, 1], confusion_matrix[1, 0], confusion_matrix[1, 1]\n",
    "missclass_acc = 100 * ((FP + FN) / (TN + FP + FN + TP))\n",
    "f'The validation set error is {missclass_acc:.2f}%.'\n",
    "\n",
    "'-----------------------'\n",
    "\n",
    "f_train, df_test = train_test_split(df, test_size=0.5, random_state=43)\n",
    "df_X_train = df_train[descriptiveColumns]\n",
    "df_Y_train = df_train[targetColumn]\n",
    "df_X_test = df_test[descriptiveColumns]\n",
    "df_Y_test = df_test[targetColumn]\n",
    "\n",
    "model = sm.Logit(df_Y_train, df_X_train)\n",
    "fitted = model.fit()\n",
    "\n",
    "sr_Y_pred = fitted.predict(df_X_test)\n",
    "df_Y_test_and_pred = pd.DataFrame({\n",
    "    'Observed': df_Y_test['default01'],\n",
    "    'Predicted': np.where(sr_Y_pred > 0.5, 1, 0),\n",
    "})\n",
    "df_confusion, df_confusion_pct = createConfusionMatrixFromOutOfSampleData(df=df_Y_test_and_pred, binaryMap={0: 'less than median', 1: 'greater than median'})\n",
    "confusion_matrix = df_confusion.to_numpy()\n",
    "TN, FP, FN, TP = confusion_matrix[0, 0], confusion_matrix[0, 1], confusion_matrix[1, 0], confusion_matrix[1, 1]\n",
    "missclass_acc = 100 * ((FP + FN) / (TN + FP + FN + TP))\n",
    "f'The validation set error is {missclass_acc:.2f}%.'\n",
    "\n",
    "'-----------------------'\n",
    "\n",
    "f_train, df_test = train_test_split(df, test_size=0.5, random_state=44)\n",
    "df_X_train = df_train[descriptiveColumns]\n",
    "df_Y_train = df_train[targetColumn]\n",
    "df_X_test = df_test[descriptiveColumns]\n",
    "df_Y_test = df_test[targetColumn]\n",
    "\n",
    "model = sm.Logit(df_Y_train, df_X_train)\n",
    "fitted = model.fit()\n",
    "\n",
    "sr_Y_pred = fitted.predict(df_X_test)\n",
    "df_Y_test_and_pred = pd.DataFrame({\n",
    "    'Observed': df_Y_test['default01'],\n",
    "    'Predicted': np.where(sr_Y_pred > 0.5, 1, 0),\n",
    "})\n",
    "df_confusion, df_confusion_pct = createConfusionMatrixFromOutOfSampleData(df=df_Y_test_and_pred, binaryMap={0: 'less than median', 1: 'greater than median'})\n",
    "confusion_matrix = df_confusion.to_numpy()\n",
    "TN, FP, FN, TP = confusion_matrix[0, 0], confusion_matrix[0, 1], confusion_matrix[1, 0], confusion_matrix[1, 1]\n",
    "missclass_acc = 100 * ((FP + FN) / (TN + FP + FN + TP))\n",
    "f'The validation set error is {missclass_acc:.2f}%.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The average validation set error is $2.67 \\%$, so adding $\\mathrm{student}$ as an independent variable does not seem to help reducing the validation set error.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
