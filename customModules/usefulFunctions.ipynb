{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T20:01:16.151939Z",
     "start_time": "2020-02-09T20:01:15.564069Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt_func\n",
    "from sklearn.model_selection import LeaveOneOut as LOO\n",
    "from sklearn.linear_model import LinearRegression as LinReg\n",
    "import statsmodels.api as sm_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T20:01:16.156589Z",
     "start_time": "2020-02-09T20:01:16.153289Z"
    }
   },
   "outputs": [],
   "source": [
    "def removeColumnsFromList(df, columnsToRemove):\n",
    "    \"\"\"\n",
    "    Return a list of columns names excluding the names in the list \n",
    "    `columnsToKeep`.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas.core.frame.DataFrame\n",
    "            The DataFrame used to produce the list of column names. \n",
    "        \n",
    "        columnsToRemove: iterable\n",
    "            An iterable object that has the names as elements that\n",
    "            will be excluded from the returned list.\n",
    "    Returns:\n",
    "        list: The aforementioned column names.\n",
    "    \"\"\"\n",
    "    columns = df.columns.tolist()\n",
    "    for column in columnsToRemove:\n",
    "        try:\n",
    "            columns.remove(column)\n",
    "        except ValueError as err:\n",
    "            if not 'list.remove(x): x not in list' in str(err):\n",
    "                raise\n",
    "\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We could easily reproduce these type of plots <a href=\"https://www.statsmodels.org/dev/examples/notebooks/generated/regression_plots.html\">very closely</a>, with <a href=\"http://www.statsmodels.org/dev/examples/notebooks/generated/regression_diagnostics.html\">additional regression diagnostics</a>, using the <code>statsmodels</code> library, however I would like to have more control, so I decided to plot most of them manually using <code>matplotlib</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T20:01:16.259592Z",
     "start_time": "2020-02-09T20:01:16.158639Z"
    }
   },
   "outputs": [],
   "source": [
    "def createResidualPlots(df_X, df_Y, fitted_model, list_of_indices=[], width=7, height=3):\n",
    "    \"\"\"\n",
    "    This function returns various residual plots for the fitted model.\n",
    "    \n",
    "    For linear regressions, the first two plots are plots of the \n",
    "    residuals and the square root of the absolute standardized residuals\n",
    "    vs the predictor. For the multiple regression fit, we instead plots \n",
    "    the residuals and the square root of the absolute standardized \n",
    "    residuals vs the fitted values. The third plot is a QQ plot of the\n",
    "    quantiles of the standardized residuals vs the quantiles of the \n",
    "    normal distribution, and a 45-degree reference line is also plotted \n",
    "    for comparison (see also \n",
    "    https://seankross.com/2016/02/29/A-Q-Q-Plot-Dissection-Kit.html). The \n",
    "    final plot is a leverage plot of the standardized residuals.\n",
    "    \n",
    "    Args:\n",
    "        df_X: pandas.core.frame.DataFrame\n",
    "            The DataFrame should hold the data of independent variables\n",
    "            (including a column for the 'Intercept' set equal to one).\n",
    "            Each row in the DataFrame represents an individual sample \n",
    "            point, with the successive columns corresponding to the \n",
    "            independent variables and their specific values for that \n",
    "            sample point. \n",
    "        df_Y: pandas.core.frame.DataFrame or pandas.core.series.Series\n",
    "            This should be a pandas Series of DataFrame of one column,\n",
    "            which holds the data of the dependent variable.        \n",
    "        fitted_model: statsmodels.regression.linear_model.RegressionResultsWrapper\n",
    "            This statsmodels class summarizes the fit of a linear regression model\n",
    "            that has been fitted with df_X and df_Y.\n",
    "        list_of_indices: list, default list()\n",
    "            A list that hold indices indicating which data point(s) want to \n",
    "            be colored differently to distinguish those point(s) from the \n",
    "            rest of the data.\n",
    "        width: float, default 7\n",
    "            The width of each subplot.\n",
    "        height: float, default 3\n",
    "            The height of each subplot.\n",
    "    \"\"\" \n",
    "    descriptiveColumns = df_X.columns.tolist()\n",
    "    try:\n",
    "        descriptiveColumns.remove('Intercept')\n",
    "    except ValueError as err:\n",
    "        if not 'list.remove(x): x not in list' in str(err):\n",
    "            raise\n",
    "            \n",
    "    assert len(descriptiveColumns) >= 1, f'descriptiveColumns = {descriptiveColumns}'\n",
    "    approach = 'simple' if len(descriptiveColumns) == 1 else 'multivariable'\n",
    "    \n",
    "    sr_Y_hat = fitted_model.predict(df_X)\n",
    "    residual = np.squeeze(df_Y.to_numpy()) - sr_Y_hat\n",
    "    from sklearn import preprocessing\n",
    "    standardized_residual = preprocessing.scale(residual)\n",
    "    \n",
    "    X = df_X.to_numpy()\n",
    "    try:\n",
    "        H = X @ np.linalg.inv(X.transpose() @ X) @ X.transpose()\n",
    "        leverage = H.diagonal() \n",
    "    except np.linalg.LinAlgError as err:\n",
    "        if not 'Singular matrix' in str(err):\n",
    "            raise\n",
    "            \n",
    "        leverage = None\n",
    "    \n",
    "    numberOfSubplots = 3 if leverage is None else 4\n",
    "    fig, axes = plt_func.subplots(numberOfSubplots, 1, constrained_layout=True, figsize=(width, height*numberOfSubplots))\n",
    "    if approach == 'simple':\n",
    "        descriptive = descriptiveColumns[0]\n",
    "        X_plot = df_X[descriptive].to_numpy()\n",
    "    else:\n",
    "        descriptive = 'fitted values'\n",
    "        X_plot = sr_Y_hat.to_numpy()\n",
    "        \n",
    "    mask_special_indices = np.zeros(residual.shape[0], dtype=bool)\n",
    "    mask_special_indices[list_of_indices] = True\n",
    "    \n",
    "    from matplotlib import colors\n",
    "    default_colors = plt_func.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    cmap = colors.ListedColormap(default_colors[:2])\n",
    "    \n",
    "    _ = axes[0].scatter(x=X_plot, y=residual, c=mask_special_indices, cmap=cmap)\n",
    "    _ = axes[0].set_xlabel(descriptive)\n",
    "    _ = axes[0].set_ylabel('residuals')\n",
    "    _ = axes[0].set_title(f'residual plot for the linear regression')\n",
    "    \n",
    "    _ = axes[1].scatter(x=X_plot, y=np.absolute(standardized_residual)**0.5, c=mask_special_indices, cmap=cmap)\n",
    "    _ = axes[1].set_xlabel(descriptive)\n",
    "    _ = axes[1].set_ylabel(r'$\\sqrt{\\left|\\mathrm{standardized \\,\\,\\, residuals}\\right|}$')\n",
    "    _ = axes[1].set_title(r'$\\sqrt{\\left|\\mathrm{standardized \\,\\,\\, residuals}\\right|}$ for the linear regression')\n",
    "        \n",
    "    n = sr_Y_hat.shape[0] + 1\n",
    "    q_list = np.linspace(start=1/n, stop=1, num=n)\n",
    "    quantiles_data = np.sort(standardized_residual)\n",
    "    from scipy import stats\n",
    "    quantiles_theoretical = stats.norm.ppf(q_list)[:-1]  # remove infinity from array\n",
    "    _ = axes[2].scatter(x=quantiles_theoretical, y=quantiles_data, c=mask_special_indices, cmap=cmap)\n",
    "    x_min, x_max = axes[2].get_xlim()\n",
    "    y_min, y_max = axes[2].get_ylim()\n",
    "    axes[2].plot((x_min, x_max), (y_min, y_max), color='black', label='45-degree line')\n",
    "    _ = axes[2].set_xlabel('normal distribution quantiles')\n",
    "    _ = axes[2].set_ylabel('standardized residuals quantiles')\n",
    "    _ = axes[2].set_title('normal qq plot')\n",
    "    _ = axes[2].legend()\n",
    "    \n",
    "    if not leverage is None:\n",
    "        _ = axes[3].scatter(x=leverage, y=standardized_residual, c=mask_special_indices, cmap=cmap)\n",
    "        _ = axes[3].set_xlabel('leverage')\n",
    "        _ = axes[3].set_ylabel('standardized residuals')\n",
    "        _ = axes[3].set_title(f'standardized residuals vs leverage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T20:01:16.358488Z",
     "start_time": "2020-02-09T20:01:16.264794Z"
    }
   },
   "outputs": [],
   "source": [
    "def createSimpleLinearRegressionPlot(df_X, df_Y, fitted_model, alpha=0.05, width=8, height=3):\n",
    "    \"\"\"\n",
    "    This function returns a scatter plot of the response and the predictor. \n",
    "    Furthermore, the simple linear regression line is shown with an \n",
    "    associated confidence and prediction interval of 1-alpha.\n",
    "    \n",
    "    Args:\n",
    "        df_X: pandas.core.frame.DataFrame\n",
    "            The DataFrame should hold the data of the independent variable.\n",
    "            Each row in the DataFrame represents an individual sample point.\n",
    "        df_Y: pandas.core.frame.DataFrame\n",
    "            This should be a DataFrame of one column,\n",
    "            which holds the data of the dependent variable.            \n",
    "        fitted_model: statsmodels.regression.linear_model.RegressionResultsWrapper\n",
    "            This statsmodels class summarizes the fit of a linear regression model\n",
    "            that has been fitted with df_X and df_Y.\n",
    "        alpha: float, default 0.05\n",
    "            This prediction and confidence intervals that are being shown are\n",
    "            of 1-alpha (e.g., alpha=0.05 corresponds to 95% confidence).\n",
    "        width: float, default 8\n",
    "            The width of the plot.\n",
    "        height: float, default 3\n",
    "            The height of the plot.        \n",
    "    \"\"\" \n",
    "    \n",
    "    assert df_Y.shape[1] == 1\n",
    "    dependent = df_Y.columns[0]\n",
    "    descriptiveColumns = df_X.columns.tolist()\n",
    "    try:\n",
    "        descriptiveColumns.remove('Intercept')\n",
    "        contains_intercept = True\n",
    "    except ValueError as err:\n",
    "        if not 'list.remove(x): x not in list' in str(err):\n",
    "            raise\n",
    "\n",
    "        contains_intercept = False\n",
    "        \n",
    "    independent = descriptiveColumns[0]\n",
    "    independent_arr = np.linspace(start=df_X[independent].min(), stop=df_X[independent].max(), num=df_X.shape[0])\n",
    "    if contains_intercept:\n",
    "        df_X_pred = pd.DataFrame({\n",
    "            'Intercept': np.ones(shape=(df_X.shape[0], ), dtype=int), \n",
    "            independent: independent_arr,\n",
    "        }, columns = ['Intercept', independent])\n",
    "    else:\n",
    "        df_X_pred = pd.DataFrame({independent: independent_arr})\n",
    "        \n",
    "    sr_Y_pred = fitted_model.predict(df_X_pred)\n",
    "    \n",
    "    # get prediction intervals\n",
    "    try:\n",
    "        from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "        std_err_prediction, lower_pred_int, upper_pred_int = wls_prediction_std(fitted_model, exog=df_X_pred, alpha=alpha)\n",
    "    except AttributeError as err:\n",
    "        if not '\\'float\\' object has no attribute \\'sqrt\\'' in str(err):\n",
    "            raise\n",
    "\n",
    "        std_err_prediction, lower_pred_int, upper_pred_int = None, None, None\n",
    "\n",
    "    # get confidence intervals\n",
    "    try:\n",
    "        result = fitted_model.get_prediction(df_X_pred)\n",
    "        conf_int = result.conf_int(alpha=alpha)\n",
    "        lower_conf_int, upper_conf_int = conf_int[:, 0], conf_int[:, 1]\n",
    "    except AttributeError as err:\n",
    "        if not '\\'float\\' object has no attribute \\'sqrt\\'' in str(err):\n",
    "            raise   \n",
    "\n",
    "        lower_conf_int, upper_conf_int = None, None\n",
    "        \n",
    "    fig, ax = plt_func.subplots(constrained_layout=True, figsize=(width, height))\n",
    "    _ = ax.scatter(df_X[independent], df_Y, label='training data')\n",
    "    _ = ax.plot(df_X_pred[independent], sr_Y_pred, '-', color='darkorchid', linewidth=2, label='prediction')\n",
    "    if not lower_conf_int is None and not upper_conf_int is None:\n",
    "        _ = ax.fill_between(df_X_pred[independent], lower_conf_int, upper_conf_int, color='#888888', alpha=0.4, label=f\"confidence interval ({int((1-alpha)*100)}%)\")\n",
    "    if not lower_pred_int is None and not upper_pred_int is None:\n",
    "        _ = ax.fill_between(df_X_pred[independent], lower_pred_int, upper_pred_int, color='#888888', alpha=0.1, label=f\"prediction interval ({int((1-alpha)*100)}%)\")\n",
    "\n",
    "    _ = ax.legend()\n",
    "    _ = ax.set_xlabel(independent)\n",
    "    _ = ax.set_ylabel(dependent)\n",
    "    _ = ax.set_title(f'regression of prediction vs training data')\n",
    "    _ = ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T20:01:16.460336Z",
     "start_time": "2020-02-09T20:01:16.362571Z"
    }
   },
   "outputs": [],
   "source": [
    "def createSimpleLinearRegressionPlotWithTransformation(df_X, df_Y, fitted_model, df_independent, alpha=0.05, width=8, height=3):\n",
    "    \"\"\"\n",
    "    This function returns a scatter plot of the response and the transformed\n",
    "    predictor. Furthermore, the simple linear regression line is shown with \n",
    "    an associated confidence and prediction interval of 1-alpha.\n",
    "    \n",
    "    Args:\n",
    "        df_X: pandas.core.frame.DataFrame\n",
    "            The DataFrame should hold the data of the independent variable.\n",
    "            Each row in the DataFrame represents an individual sample point.\n",
    "        df_Y: pandas.core.frame.DataFrame\n",
    "            This should be a DataFrame of one column,\n",
    "            which holds the data of the dependent variable.\n",
    "        df_independent: pd.core.frame.DataFrame\n",
    "            This DataFrame should hold the data of the independent variable before \n",
    "            the transformation has been applied to the variable, and will be \n",
    "            used to plot the regression instead of the transformed variable in\n",
    "            the DataFrame df_X.\n",
    "        fitted_model: statsmodels.regression.linear_model.RegressionResultsWrapper\n",
    "            This statsmodels class summarizes the fit of a linear regression model\n",
    "            that has been fitted with df_X and df_Y.\n",
    "        alpha: float, default 0.05\n",
    "            This prediction and confidence intervals that are being shown are\n",
    "            of 1-alpha (e.g., alpha=0.05 corresponds to 95% confidence).\n",
    "        width: float, default 8\n",
    "            The width of the plot.\n",
    "        height: float, default 3\n",
    "            The height of the plot.        \n",
    "    \"\"\" \n",
    "    \n",
    "    assert df_Y.shape[1] == 1\n",
    "    dependent = df_Y.columns[0]\n",
    "    descriptiveColumns = df_X.columns.tolist()\n",
    "    try:\n",
    "        descriptiveColumns.remove('Intercept')\n",
    "        contains_intercept = True\n",
    "    except ValueError as err:\n",
    "        if not 'list.remove(x): x not in list' in str(err):\n",
    "            raise\n",
    "\n",
    "        contains_intercept = False  \n",
    "    \n",
    "    independent = descriptiveColumns[0]\n",
    "    independent_arr = np.linspace(start=df_X[independent].min(), stop=df_X[independent].max(), num=df_X.shape[0])\n",
    "    if contains_intercept:\n",
    "        df_X_pred = pd.DataFrame({\n",
    "            'Intercept': np.ones(shape=(df_X.shape[0], ), dtype=int), \n",
    "            independent: independent_arr,\n",
    "        }, columns = ['Intercept', independent])\n",
    "    else:\n",
    "        df_X_pred = pd.DataFrame({independent: independent_arr})\n",
    "    \n",
    "    sr_Y_pred = fitted_model.predict(df_X_pred)\n",
    "\n",
    "    # get prediction intervals\n",
    "    try:\n",
    "        from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "        std_err_prediction, lower_pred_int, upper_pred_int = wls_prediction_std(fitted_model, exog=df_X_pred, alpha=alpha)\n",
    "    except AttributeError as err:\n",
    "        if not '\\'float\\' object has no attribute \\'sqrt\\'' in str(err):\n",
    "            raise\n",
    "\n",
    "        std_err_prediction, lower_pred_int, upper_pred_int = None, None, None\n",
    "\n",
    "    # get confidence intervals\n",
    "    try:\n",
    "        result = fitted_model.get_prediction(df_X_pred)\n",
    "        conf_int = result.conf_int(alpha=alpha)\n",
    "        lower_conf_int, upper_conf_int = conf_int[:, 0], conf_int[:, 1]\n",
    "    except AttributeError as err:\n",
    "        if not '\\'float\\' object has no attribute \\'sqrt\\'' in str(err):\n",
    "            raise   \n",
    "\n",
    "        lower_conf_int, upper_conf_int = None, None\n",
    "\n",
    "    independent = df_independent.columns[0]\n",
    "#     df_X = pd.concat([df_X, df_independent], axis=1)\n",
    "    df_X_pred[independent] = np.linspace(start=df_independent.min(), stop=df_independent.max(), num=df_independent.shape[0])\n",
    "    \n",
    "    fig, ax = plt_func.subplots(constrained_layout=True, figsize=(width, height))\n",
    "    _ = ax.scatter(df_independent, df_Y, label='pre-transformed training data')\n",
    "    _ = ax.plot(df_X_pred[independent], sr_Y_pred, '-', color='darkorchid', linewidth=2, label='prediction')\n",
    "    if not lower_conf_int is None and not upper_conf_int is None:\n",
    "        _ = ax.fill_between(df_X_pred[independent], lower_conf_int, upper_conf_int, color='#888888', alpha=0.4, label=f\"confidence interval ({int((1-alpha)*100)}%)\")\n",
    "    if not lower_pred_int is None and not upper_pred_int is None:\n",
    "        _ = ax.fill_between(df_X_pred[independent], lower_pred_int, upper_pred_int, color='#888888', alpha=0.1, label=f\"prediction interval ({int((1-alpha)*100)}%)\")\n",
    "\n",
    "    _ = ax.legend()\n",
    "    _ = ax.set_xlabel(independent)\n",
    "    _ = ax.set_ylabel(dependent)\n",
    "    _ = ax.set_title('regression of prediction vs pre-transformed training data')\n",
    "    _ = ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T20:01:16.578806Z",
     "start_time": "2020-02-09T20:01:16.465813Z"
    }
   },
   "outputs": [],
   "source": [
    "def createPolynomialLinearRegressionPlot(df_X, df_Y, fitted_model, polynomialMap, df_independent=None, alpha=0.05, width=8, height=3):\n",
    "    \"\"\"\n",
    "    This function returns a scatter plot of the response and the predictor. \n",
    "    Furthermore, the polynomial regression line is shown with an \n",
    "    associated confidence and prediction interval of 1-alpha.\n",
    "    \n",
    "    Args:\n",
    "        df_X: pandas.core.frame.DataFrame\n",
    "            The DataFrame should hold the data of the exponentiated variable\n",
    "            used for the polynomial regression. Each row in the DataFrame \n",
    "            represents an individual sample point.\n",
    "        df_Y: pandas.core.frame.DataFrame\n",
    "            This should be a DataFrame of one column,\n",
    "            which holds the data of the dependent variable.\n",
    "        fitted_model: statsmodels.regression.linear_model.RegressionResultsWrapper\n",
    "            This statsmodels class summarizes the fit of a linear regression model\n",
    "            that has been fitted with df_X and df_Y.\n",
    "        polynomialMap: dict\n",
    "            This argument is used when plotting a polynomial regression. It is a \n",
    "            dictionary that must contain the column names of the DataFrame X \n",
    "            (excluding the intercept if the model is fitted with an intercept) as\n",
    "            keys with their associated polynomial degrees as values. For instance,\n",
    "            let us imagine that the model being fitted takes the form\n",
    "                y = a + b*x^2 + c*x^5\n",
    "            then the polynomialMap argument should be\n",
    "                mapping_powers = {\n",
    "                    'b': 2,\n",
    "                    'c': 5,\n",
    "                }\n",
    "        df_independent: pd.core.frame.DataFrame, default None\n",
    "            This argument is only used when plotting a polynomial regression without\n",
    "            a term of degree 1. The DataFrame should hold the data of the independent \n",
    "            variable exponentiated to the power 1, and will be used to plot the \n",
    "            regression instead of the transformed variable in the DataFrame df_X.\n",
    "            A string describing the independent variable of the regression. \n",
    "        alpha: float, default 0.05\n",
    "            This prediction and confidence intervals that are being shown are\n",
    "            of 1-alpha (e.g., alpha=0.05 corresponds to 95% confidence).\n",
    "        width: float, default 8\n",
    "            The width of the plot.\n",
    "        height: float, default 3\n",
    "            The height of the plot.        \n",
    "    \"\"\" \n",
    "    \n",
    "    assert df_Y.shape[1] == 1\n",
    "    dependent = df_Y.columns[0]\n",
    "    descriptiveColumns = df_X.columns.tolist()\n",
    "    try:\n",
    "        descriptiveColumns.remove('Intercept')\n",
    "        contains_intercept = True\n",
    "    except ValueError as err:\n",
    "        if not 'list.remove(x): x not in list' in str(err):\n",
    "            raise\n",
    "\n",
    "        contains_intercept = False\n",
    "        \n",
    "    assert len(descriptiveColumns) == len(polynomialMap), f'descriptiveColumns = {descriptiveColumns} and polynomialMap = {polynomialMap}'\n",
    "    sortedMap = [(key, polynomialMap[key]) for key in descriptiveColumns if key in polynomialMap]\n",
    "    _, polynomialTuple = zip(*sortedMap)\n",
    "    first_degree_polynomial_term = True if 1 in polynomialTuple else False\n",
    "        \n",
    "    if contains_intercept:\n",
    "        df_X_pred = pd.DataFrame({'Intercept': np.ones(df_X.shape[0], dtype=int)})\n",
    "    else:\n",
    "        df_X_pred = pd.DataFrame(index=range(0, df_X.shape[0]))\n",
    "\n",
    "    if first_degree_polynomial_term:\n",
    "        index_independent = polynomialTuple.index(1)\n",
    "        independent = descriptiveColumns[index_independent]\n",
    "        independent_arr = np.linspace(start=df_X[independent].min(), stop=df_X[independent].max(), num=df_X.shape[0])\n",
    "        for index, power in enumerate(polynomialTuple):\n",
    "            column = descriptiveColumns[index]\n",
    "            if index == index_independent: \n",
    "                df_X_pred[independent] = independent_arr\n",
    "            else:\n",
    "                df_X_pred[column] = independent_arr**power\n",
    "    else:\n",
    "        assert not df_independent is None\n",
    "        independent = df_independent.columns[0]\n",
    "        sr_X_independent = df_independent[independent]\n",
    "        assert isinstance(sr_X_independent, pd.core.series.Series), f'type(sr_X_independent) = {type(sr_X_independent)}'\n",
    "        X_pred_independent = np.linspace(start=sr_X_independent.min(), stop=sr_X_independent.max(), num=df_X.shape[0])\n",
    "        for index, power in enumerate(polynomialTuple):\n",
    "            column = descriptiveColumns[index]\n",
    "            df_X_pred[column] = X_pred_independent**power\n",
    "\n",
    "    sr_Y_pred = fitted_model.predict(df_X_pred)\n",
    "\n",
    "    # get prediction intervals\n",
    "    try:\n",
    "        from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "        std_err_prediction, lower_pred_int, upper_pred_int = wls_prediction_std(fitted_model, exog=df_X_pred, alpha=alpha)\n",
    "    except AttributeError as err:\n",
    "        if not '\\'float\\' object has no attribute \\'sqrt\\'' in str(err):\n",
    "            raise\n",
    "\n",
    "        std_err_prediction, lower_pred_int, upper_pred_int = None, None, None\n",
    "\n",
    "    # get confidence intervals\n",
    "    try:\n",
    "        result = fitted_model.get_prediction(df_X_pred)\n",
    "        conf_int = result.conf_int(alpha=alpha)\n",
    "        lower_conf_int, upper_conf_int = conf_int[:, 0], conf_int[:, 1]\n",
    "    except AttributeError as err:\n",
    "        if not '\\'float\\' object has no attribute \\'sqrt\\'' in str(err):\n",
    "            raise   \n",
    "\n",
    "        lower_conf_int, upper_conf_int = None, None\n",
    "\n",
    "    if not first_degree_polynomial_term:\n",
    "        df_X = df_X.copy()\n",
    "        df_X[independent] = sr_X_independent\n",
    "        df_X_pred[independent] = X_pred_independent\n",
    "        \n",
    "    fig, ax = plt_func.subplots(constrained_layout=True, figsize=(width, height))\n",
    "    _ = ax.scatter(df_X[independent], df_Y, label='training data')\n",
    "    _ = ax.plot(df_X_pred[independent], sr_Y_pred, '-', color='darkorchid', linewidth=2, label='prediction')\n",
    "    if not lower_conf_int is None and not upper_conf_int is None:\n",
    "        _ = ax.fill_between(df_X_pred[independent], lower_conf_int, upper_conf_int, color='#888888', alpha=0.4, label=f\"confidence interval ({int((1-alpha)*100)}%)\")\n",
    "    if not lower_pred_int is None and not upper_pred_int is None:\n",
    "        _ = ax.fill_between(df_X_pred[independent], lower_pred_int, upper_pred_int, color='#888888', alpha=0.1, label=f\"prediction interval ({int((1-alpha)*100)}%)\")\n",
    "\n",
    "    _ = ax.legend()\n",
    "    _ = ax.set_xlabel(independent)\n",
    "    _ = ax.set_ylabel(dependent)\n",
    "    _ = ax.set_title('regression of prediction vs training data')\n",
    "    _ = ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T20:01:16.675587Z",
     "start_time": "2020-02-09T20:01:16.583461Z"
    }
   },
   "outputs": [],
   "source": [
    "def createConfusionMatrixFromLogisticModel(fitted_model, threshold=0.5, binaryMap={0: 0, 1: 1}):\n",
    "    \"\"\"\n",
    "    This function returns two confusion matrices in terms of absolute \n",
    "    numbers and percentages, respectively, based on in-sample data fitted\n",
    "    with a logistic regression model.\n",
    "    \n",
    "    Args:\n",
    "        fitted_model: statsmodels.discrete.discrete_model.LogitResults\n",
    "            This statsmodels class summarizes the fit of a logistic \n",
    "            regression model.\n",
    "        threshold: float, default 0.5\n",
    "            Number between 0 and 1. Threshold above which a prediction\n",
    "            is considered 1 and below which a prediction is considered 0.\n",
    "        binaryMap: dictionary, default {0: 0, 1: 1}\n",
    "            A mapping of the binary 0 and 1 quantative variables to their\n",
    "            associated qualitative name.\n",
    "    Returns:\n",
    "        tuple of two pandas.core.frame.DataFrames: The aforementioned confusion\n",
    "            matrices, in absolute values and percentages respectively.\n",
    "    \"\"\"\n",
    "   \n",
    "    # pred_table[i,j] refers to the number of times “i” was observed\n",
    "    # and the model predicted “j”. Correct predictions are along the diagonal.\n",
    "    confusion = fitted_model.pred_table(threshold=0.5).astype(int)\n",
    "    \n",
    "    index = pd.MultiIndex.from_tuples([('Observed', binaryMap[0]), ('Observed', binaryMap[1])])\n",
    "    columns = pd.MultiIndex.from_tuples([('Predicted', binaryMap[0]), ('Predicted', binaryMap[1])])\n",
    "    df_confusion = pd.DataFrame(confusion, columns=columns, index=index)\n",
    "    \n",
    "    # TN, FP, FN and TP denote the 'true negative', 'false positive',\n",
    "    # 'false negative' and 'true positive', respectively.\n",
    "    TN, FP, FN, TP = confusion[0, 0], confusion[0, 1], confusion[1, 0], confusion[1, 1]\n",
    "    TNR = TN / (TN + FP)  # true negative rate\n",
    "    FPR = FP / (TN + FP)  # false positive rate\n",
    "    FNR = FN / (TP + FN)  # false negative rate\n",
    "    TPR = TP / (TP + FN)  # true positive rate\n",
    "    confusion_pct = 100 * np.array([\n",
    "        [TNR, FPR],\n",
    "        [FNR, TPR]\n",
    "    ])\n",
    "    index_pct = pd.MultiIndex.from_tuples([('Observed (%)', binaryMap[0]), ('Observed (%)', binaryMap[1])])\n",
    "    columns_pct = pd.MultiIndex.from_tuples([('Predicted (%)', binaryMap[0]), ('Predicted (%)', binaryMap[1])])\n",
    "    df_confusion_pct = pd.DataFrame(confusion_pct, columns=columns_pct, index=index_pct)\n",
    "    \n",
    "    return df_confusion, df_confusion_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T20:01:16.767915Z",
     "start_time": "2020-02-09T20:01:16.683611Z"
    }
   },
   "outputs": [],
   "source": [
    "def createConfusionMatrixFromOutOfSampleData(df, binaryMap={0: 0, 1: 1}):\n",
    "    \"\"\"\n",
    "    This function returns two confusion matrices in terms of absolute \n",
    "    numbers and percentages, respectively, based on out-of-sample data.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas.core.frame.DataFrame\n",
    "            The DataFrame used to produce the confusion matrices. It \n",
    "            should have a column named 'Observed' and one column named\n",
    "            'Predicted' which contains the binary values (0 or 1) of \n",
    "            the observed and predicted data, respectively.\n",
    "        binaryMap: dictionary, default {0: 0, 1: 1}\n",
    "            A mapping of the binary 0 and 1 quantative variables to their\n",
    "            associated qualitative name.\n",
    "    Returns:\n",
    "        tuple of two pandas.core.frame.DataFrames: The aforementioned confusion\n",
    "            matrices, in absolute values and percentages respectively.\n",
    "    \"\"\"\n",
    "    \n",
    "    TP = np.sum(np.where((df['Observed'] == 1) & (df['Predicted'] == 1), 1, 0))\n",
    "    TN = np.sum(np.where((df['Observed'] == 0) & (df['Predicted'] == 0), 1, 0))\n",
    "    FP = np.sum(np.where((df['Observed'] == 0) & (df['Predicted'] == 1), 1, 0))\n",
    "    FN = np.sum(np.where((df['Observed'] == 1) & (df['Predicted'] == 0), 1, 0))\n",
    "    \n",
    "    confusion = np.array([\n",
    "        [TN, FP],\n",
    "        [FN, TP]\n",
    "    ])\n",
    "    \n",
    "    index = pd.MultiIndex.from_tuples([('Observed', binaryMap[0]), ('Observed', binaryMap[1])])\n",
    "    columns = pd.MultiIndex.from_tuples([('Predicted', binaryMap[0]), ('Predicted', binaryMap[1])])\n",
    "    df_confusion = pd.DataFrame(confusion, columns=columns, index=index)\n",
    "    \n",
    "    TNR = TN / (TN + FP)  # true negative rate\n",
    "    FPR = FP / (TN + FP)  # false positive rate\n",
    "    FNR = FN / (TP + FN)  # false negative rate\n",
    "    TPR = TP / (TP + FN)  # true positive rate\n",
    "    confusion_pct = 100 * np.array([\n",
    "        [TNR, FPR],\n",
    "        [FNR, TPR]\n",
    "    ])\n",
    "    index_pct = pd.MultiIndex.from_tuples([('Observed (%)', binaryMap[0]), ('Observed (%)', binaryMap[1])])\n",
    "    columns_pct = pd.MultiIndex.from_tuples([('Predicted (%)', binaryMap[0]), ('Predicted (%)', binaryMap[1])])\n",
    "    df_confusion_pct = pd.DataFrame(confusion_pct, columns=columns_pct, index=index_pct)\n",
    "    \n",
    "    return df_confusion, df_confusion_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T20:01:16.849870Z",
     "start_time": "2020-02-09T20:01:16.773649Z"
    }
   },
   "outputs": [],
   "source": [
    "def stepFunctionChooseOptimalCuts(df_X, df_Y, total_cuts):\n",
    "    \"\"\"\n",
    "    This function fits a step function to a data set, and \n",
    "    performs cross-validation to choose the optimal number\n",
    "    of cuts. \n",
    "    \n",
    "    Args:\n",
    "        df_X: pandas.core.frame.DataFrame\n",
    "            This should be a DataFrame of one column,\n",
    "            which holds the data of the independent \n",
    "            variable used to fit a step function. Each \n",
    "            row in the DataFrame represents an individual\n",
    "            sample point.\n",
    "        df_Y: pandas.core.frame.DataFrame\n",
    "            This should be a DataFrame of one column,\n",
    "            which holds the data of the dependent variable.\n",
    "        total_cuts: int\n",
    "            This number represents the total numbers of cuts\n",
    "            this function will use to obtain the test MSE.\n",
    "    \"\"\"\n",
    "    assert df_X.shape[1] == 1, f'df_X.shape = {df_X.shape}'\n",
    "    loocv = LOO() # LeaveOneOut\n",
    "    results = {}\n",
    "\n",
    "    total_bins = total_cuts + 1\n",
    "    independent = df_X.columns[0]\n",
    "    n = df_X.shape[0]\n",
    "    for no_bins in range(2, total_bins + 1):\n",
    "        MSE = 0\n",
    "        counter = 0\n",
    "        for train_index, test_index in loocv.split(df_X):\n",
    "            df_X_train, df_X_test = df_X.iloc[train_index], df_X.iloc[test_index]\n",
    "            df_Y_train, df_Y_test = df_Y.iloc[train_index], df_Y.iloc[test_index]\n",
    "\n",
    "            df_cut_train, bins_train = pd.cut(df_X_train[independent], bins=no_bins, retbins=True, right=False)\n",
    "            df_steps_train = pd.concat([df_X_train, df_cut_train, df_Y_train], axis=1)\n",
    "            df_steps_train.columns = ['independent', 'independent_cuts', 'dependent']\n",
    "\n",
    "            # Create dummy variables for the age groups\n",
    "            df_steps_dummies_train = pd.get_dummies(df_steps_train['independent_cuts'])\n",
    "            # delete first column (see footnote on page 269 of 'An Introduction to Statistical Learning')\n",
    "            df_steps_dummies_train = df_steps_dummies_train.drop(df_steps_dummies_train.columns[0], axis=1)  \n",
    "            df_steps_dummies_train = sm_func.add_constant(df_steps_dummies_train)\n",
    "\n",
    "            fitted = sm_func.GLM(df_Y_train, df_steps_dummies_train).fit()\n",
    "\n",
    "            # Put the test data in the same bins as the training data.\n",
    "            bin_mapping_test = str(np.digitize(df_X_test, bins_train)[0, 0])\n",
    "            # start range from 2 because we ignore the first column (see footnote on page 269 of 'An Introduction to Statistical Learning')\n",
    "            df_X_test2 = pd.DataFrame({str(i): [0] for i in range(2, no_bins + 1)})\n",
    "            df_X_test2.insert(0, 'Intercept', 1)\n",
    "            if bin_mapping_test != '1' and bin_mapping_test != '0': # bin_mapping_test == '0' happens when the test value is smaller than any of the bins, so gets put in the zeroth bin\n",
    "                # if bin_mapping_test == '1', then the value is put in the first bin, that we ignore (see footnote on page 269 of 'An Introduction to Statistical Learning')\n",
    "                # because when bin_mapping_test == '1', equation (7.5) on page 269 reduces to:\n",
    "                # y_i = \\beta_0\n",
    "                # so this case is already absorbed in the intercept, and we can safely ignore it\n",
    "                try:\n",
    "                    assert bin_mapping_test in df_X_test2.columns, f'bin_mapping_test = {bin_mapping_test}, df_X_test.iloc[0, 0] = {df_X_test.iloc[0, 0]} and df_X_test2.columns = {df_X_test2.columns}'\n",
    "                except AssertionError:\n",
    "                    if bin_mapping_test != str(int(no_bins + 1)): # bin_mapping_test == str(no_bins + 1) happens when the test value is larger than any of the bins, so gets put in the (no_bins + 1) bin\n",
    "                        raise\n",
    "                        \n",
    "                    bin_mapping_test = str(no_bins)\n",
    "                    \n",
    "                df_X_test2[bin_mapping_test] = 1\n",
    "\n",
    "            sr_Y_pred = fitted.predict(df_X_test2)\n",
    "            MSE += (df_Y_test.iloc[0, 0] - sr_Y_pred.iloc[0])**2 \n",
    "\n",
    "        cuts = no_bins - 1\n",
    "        results[cuts] = MSE / n\n",
    "        \n",
    "    cuts_lst = list(results.keys())\n",
    "    MSE_lst = list(results.values())\n",
    "    std_dev = np.std(MSE_lst)\n",
    "    min_mse = min(results, key=results.get)\n",
    "    min_val = results[min_mse]\n",
    "    cuts_lst.remove(min_mse)  # we plot these 'best results' with a different marker\n",
    "    MSE_lst.remove(min_val)  # we plot these 'best results' with a different marker\n",
    "\n",
    "    numberOfSubplots = 1\n",
    "    fig, ax1 = plt_func.subplots(1, numberOfSubplots, constrained_layout=True, figsize=(8*numberOfSubplots, 4))\n",
    "    _ = ax1.scatter(cuts_lst, MSE_lst)\n",
    "    _ = ax1.scatter(min_mse, min_val, marker='x', s=40, c=plt_func.rcParams['axes.prop_cycle'].by_key()['color'][0], label='smallest MSE')\n",
    "    min_val_2std = min_val - 0.2*std_dev\n",
    "    _ = ax1.axhline(y=min_val_2std, linestyle='dashed', linewidth=0.5, c=plt_func.rcParams['axes.prop_cycle'].by_key()['color'][1], label='0.2 standard deviation')\n",
    "    _ = ax1.axhline(y=min_val + 0.2*std_dev, linestyle='dashed', linewidth=0.5, c=plt_func.rcParams['axes.prop_cycle'].by_key()['color'][1])\n",
    "    _ = ax1.set_xlabel(r'number of cut points')\n",
    "    _ = ax1.set_ylabel('Cross-validation MSE')\n",
    "    _ = ax1.legend()\n",
    "    \n",
    "    diff = max(MSE_lst) - min_val_2std\n",
    "    if diff > 70:\n",
    "        _ = ax1.set_ylim(ymin = min_val_2std - (min_val_2std % 10) - 10)\n",
    "    elif diff > 40:\n",
    "        _ = ax1.set_ylim(ymin = min_val_2std - (min_val_2std % 10))\n",
    "    else:\n",
    "        _ = ax1.set_ylim(ymin = min_val_2std - (min_val_2std % 1) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T20:01:17.004060Z",
     "start_time": "2020-02-09T20:01:16.851386Z"
    }
   },
   "outputs": [],
   "source": [
    "def plotFittedStepFunction(df_X, df_Y, cuts, alpha=0.05):\n",
    "    \"\"\"\n",
    "    This function returns a scatter plot of the response and the predictor. \n",
    "    Furthermore, fitted step functions are shown with their associated \n",
    "    confidence interval of 1-alpha.\n",
    "    \n",
    "    Args:\n",
    "        df_X: pandas.core.frame.DataFrame\n",
    "            This should be a pandas DataFrame of one column, that holds\n",
    "            the data of independent variable. Each row in the DataFrame \n",
    "            represents an individual sample point.\n",
    "        df_Y: pandas.core.frame.DataFrame or pandas.core.series.Series\n",
    "            This should be a pandas DataFrame of one column,\n",
    "            which holds the data of the dependent variable. \n",
    "        cuts: int\n",
    "            Integer representing the number of cuts that should be created\n",
    "            to fit the step functions.\n",
    "        alpha: float, default 0.05\n",
    "            This prediction and confidence intervals that are being shown are\n",
    "            of 1-alpha (e.g., alpha=0.05 corresponds to 95% confidence).\n",
    "    \"\"\" \n",
    "    no_bins = cuts + 1\n",
    "    independent = df_X.columns[0]\n",
    "    dependent = df_Y.columns[0]\n",
    "\n",
    "    df_cut, bins = pd.cut(df_X[independent], bins=no_bins, retbins=True, right=False)\n",
    "    df_steps = pd.concat([df_X, df_cut, df_Y], axis=1)\n",
    "    df_steps.columns = ['independent', 'independent_cuts', 'dependent']\n",
    "\n",
    "    # Create dummy variables for the age groups\n",
    "    df_steps_dummies = pd.get_dummies(df_steps['independent_cuts'])\n",
    "    # delete first column (see footnote on page 269 of 'An Introduction to Statistical Learning')\n",
    "    df_steps_dummies = df_steps_dummies.drop(df_steps_dummies.columns[0], axis=1)  \n",
    "    df_steps_dummies = sm_func.add_constant(df_steps_dummies)\n",
    "\n",
    "    fitted_model = sm_func.GLM(df_Y, df_steps_dummies).fit()\n",
    "\n",
    "    X_pred = np.linspace(df_X.min().iloc[0], df_X.max().iloc[0], num=df_X.shape[0], endpoint=True)\n",
    "    # Put the test data in the same bins as the training data.\n",
    "    bin_mapping = np.digitize(X_pred.ravel(), bins)\n",
    "    # Get dummies, drop first dummy category, add constant\n",
    "    df_X_pred = sm_func.add_constant(pd.get_dummies(bin_mapping).drop(1, axis = 1))\n",
    "    # Predict the value of the generated ages using the linear model\n",
    "    sr_Y_pred = fitted_model.predict(df_X_pred)\n",
    "\n",
    "    numberOfSubplots = 1\n",
    "    fig, ax1 = plt_func.subplots(1, numberOfSubplots, constrained_layout=True, figsize=(8*numberOfSubplots, 4))\n",
    "    _ = ax1.scatter(df_X, df_Y, label='training data', s=0.5)\n",
    "    _ = ax1.plot(X_pred, sr_Y_pred, '-', color='darkorchid', linewidth=2, label='prediction')\n",
    "\n",
    "    # get confidence intervals\n",
    "    try:\n",
    "        result = fitted_model.get_prediction(df_X_pred)\n",
    "        conf_int = result.conf_int(alpha=alpha)\n",
    "        lower_conf_int, upper_conf_int = conf_int[:, 0], conf_int[:, 1]\n",
    "    except AttributeError as err:\n",
    "        if not '\\'float\\' object has no attribute \\'sqrt\\'' in str(err):\n",
    "            raise\n",
    "\n",
    "        lower_conf_int, upper_conf_int = None, None\n",
    "\n",
    "    if not lower_conf_int is None and not upper_conf_int is None:\n",
    "        _ = ax1.fill_between(X_pred, lower_conf_int, upper_conf_int, color='#888888', alpha=0.4, label=f\"confidence interval ({int((1-alpha)*100)}%)\")\n",
    "        \n",
    "    _ = ax1.set_xlabel(independent)\n",
    "    _ = ax1.set_ylabel(dependent)\n",
    "    _ = ax1.set_ylim(ymin = 0)\n",
    "    _ = ax1.legend()\n",
    "    _ = ax1.set_title(f'fitted step function of prediction vs training data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T20:01:17.113583Z",
     "start_time": "2020-02-09T20:01:17.009163Z"
    }
   },
   "outputs": [],
   "source": [
    "def polynomialRegressionChooseOptimalDegree(df_X, df_Y, total_degrees):\n",
    "    \"\"\"\n",
    "    This function fits a polynomial regression to a data set, \n",
    "    and performs cross-validation to choose the optimal number\n",
    "    of degrees of the polynomial fit. \n",
    "    \n",
    "    Args:\n",
    "        df_X: pandas.core.frame.DataFrame\n",
    "            This should be a DataFrame of one column,\n",
    "            which holds the data of the independent \n",
    "            variable used to fit a step function. Each \n",
    "            row in the DataFrame represents an individual\n",
    "            sample point.\n",
    "        df_Y: pandas.core.frame.DataFrame\n",
    "            This should be a DataFrame of one column,\n",
    "            which holds the data of the dependent variable.\n",
    "        total_degrees: int\n",
    "            This number represents the total numbers of degrees\n",
    "            this function will use to obtain the test MSE.\n",
    "    \"\"\"\n",
    "    assert df_X.shape[1] == 1, f\"'Intercept' should not be included\"\n",
    "    df_X = df_X.copy()\n",
    "    independent = df_X.columns[0]\n",
    "    for i in range(2, total_degrees + 1):\n",
    "        variable_name = independent + str(i)\n",
    "        df_X[variable_name] = df_X[independent]**i\n",
    "\n",
    "    MSE = np.sum((df_Y - df_Y.mean().iloc[0])**2).iloc[0] / df_Y.shape[0]\n",
    "    best_submodels = {\n",
    "        0: (['Intercept'], MSE),  # null model\n",
    "    }\n",
    "\n",
    "    n, p = df_X.shape\n",
    "    features_lst = df_X.columns.tolist()\n",
    "    results = {}\n",
    "    loocv = LOO() # LeaveOneOut\n",
    "    for k in range(1, p + 1):\n",
    "        descriptiveColumns = features_lst[:k]\n",
    "        MSE = 0\n",
    "        for train_index, test_index in loocv.split(df_X):\n",
    "            df_X_train, df_X_test = df_X[descriptiveColumns].iloc[train_index], df_X[descriptiveColumns].iloc[test_index]\n",
    "            df_Y_train, df_Y_test = df_Y.iloc[train_index], df_Y.iloc[test_index]\n",
    "\n",
    "            model = LinReg()\n",
    "            _ = model.fit(df_X_train, df_Y_train)\n",
    "\n",
    "            Y_pred = model.predict(df_X_test)\n",
    "            MSE += (df_Y_test.iloc[0, 0] - Y_pred[0, 0])**2        \n",
    "\n",
    "        results[k] = MSE / n\n",
    "\n",
    "    degrees_lst = list(results.keys())\n",
    "    MSE_lst = list(results.values())\n",
    "    std_dev = np.std(MSE_lst)\n",
    "    min_mse = min(results, key=results.get)\n",
    "    min_val = results[min_mse]\n",
    "    degrees_lst.remove(min_mse)  # we plot these 'best results' with a different marker\n",
    "    MSE_lst.remove(min_val)  # we plot these 'best results' with a different marker\n",
    "\n",
    "    numberOfSubplots = 1\n",
    "    fig, ax1 = plt_func.subplots(1, numberOfSubplots, constrained_layout=True, figsize=(8*numberOfSubplots, 4))\n",
    "    _ = ax1.scatter(degrees_lst, MSE_lst)\n",
    "    _ = ax1.scatter(min_mse, min_val, marker='x', s=40, c=plt_func.rcParams['axes.prop_cycle'].by_key()['color'][0], label='smallest MSE')\n",
    "    min_val_2std = min_val - 0.2*std_dev\n",
    "    _ = ax1.axhline(y=min_val_2std, linestyle='dashed', linewidth=0.5, c=plt_func.rcParams['axes.prop_cycle'].by_key()['color'][1], label='0.2 standard deviation')\n",
    "    _ = ax1.axhline(y=min_val + 0.2*std_dev, linestyle='dashed', linewidth=0.5, c=plt_func.rcParams['axes.prop_cycle'].by_key()['color'][1])\n",
    "    _ = ax1.set_xlabel(r'degree of polynomial $d$')\n",
    "    _ = ax1.set_ylabel('Cross-validation MSE')\n",
    "    _ = ax1.legend()\n",
    "    \n",
    "    diff = max(MSE_lst) - min_val_2std\n",
    "    if diff > 70:\n",
    "        _ = ax1.set_ylim(ymin = min_val_2std - (min_val_2std % 10) - 10)\n",
    "    elif diff > 40:\n",
    "        _ = ax1.set_ylim(ymin = min_val_2std - (min_val_2std % 10))\n",
    "    else:\n",
    "        _ = ax1.set_ylim(ymin = min_val_2std - (min_val_2std % 1) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T20:01:17.218803Z",
     "start_time": "2020-02-09T20:01:17.118149Z"
    }
   },
   "outputs": [],
   "source": [
    "def fitCubicSplines(sr_X_train, sr_X_test, df_Y_train, df_Y_test, K):\n",
    "    \"\"\"\n",
    "    This function fits a cubic spline with K knots to the training data \n",
    "    sr_X_train and df_Y_train. It returns the fitted model and the\n",
    "    mean squared error due to the test data sr_X_test and df_Y_test.\n",
    "    \n",
    "    Args:\n",
    "        sr_X_train: pandas.core.series.Series\n",
    "            The Series holding the training data associated with the\n",
    "            independent variable.\n",
    "        sr_X_test: pandas.core.series.Series\n",
    "            The Series holding the test data associated with the\n",
    "            independent variable.\n",
    "        df_Y_train: pandas.core.frame.DataFrame\n",
    "            The DataFrame olding the training data associated with the\n",
    "            dependent variable.\n",
    "        df_Y_train: pandas.core.frame.DataFrame\n",
    "            The DataFrame olding the test data associated with the\n",
    "            dependent variable.\n",
    "        K: int\n",
    "            Number of knots used to fit the cubic spline.\n",
    "    Returns:\n",
    "        tuple: (statsmodels.genmod.generalized_linear_model.GLM.fit, MSE)\n",
    "            The first element of the returned tuple is the fitted model\n",
    "            on the training data. The second element respresents the test\n",
    "            MSE.\n",
    "    \"\"\"\n",
    "    \n",
    "    # https://github.com/pydata/patsy/issues/108\n",
    "    # https://www.statsmodels.org/dev/generated/statsmodels.gam.smooth_basis.BSplines.html\n",
    "    # cubic spline with K knots uses has K+4 degrees of freedom\n",
    "    # so number of basis functions df = K + 3\n",
    "    df = K + 3\n",
    "    df_X_transformed_train = dmatrix(f'bs(sr_X_train, df={df}, include_intercept=True)',\n",
    "                                     {'sr_X_train': sr_X_train}, return_type='dataframe')\n",
    "\n",
    "    assert df_X_transformed_train.shape[1] == df + 1\n",
    "\n",
    "    # Build a regular linear model from the splines\n",
    "    fitted_model = sm.GLM(df_Y_train, df_X_transformed_train).fit()\n",
    "\n",
    "    sr_Y_pred = fitted_model.predict(dmatrix(f'bs(sr_X_test, df={df}, include_intercept=True)', \n",
    "                                       {'sr_X_test': sr_X_test}, return_type='dataframe'))\n",
    "\n",
    "    MSE = mean_squared_error(df_Y_test, sr_Y_pred)\n",
    "    \n",
    "    return fitted_model, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T20:01:17.320594Z",
     "start_time": "2020-02-09T20:01:17.223357Z"
    }
   },
   "outputs": [],
   "source": [
    "def fitNaturalCubicSplines(sr_X_train, sr_X_test, df_Y_train, df_Y_test, K):\n",
    "    \"\"\"\n",
    "    This function fits a natural cubic spline with K knots to the \n",
    "    training data sr_X_train and df_Y_train. It returns the fitted \n",
    "    model and the mean squared error due to the test data sr_X_test\n",
    "    and df_Y_test.\n",
    "    \n",
    "    Args:\n",
    "        sr_X_train: pandas.core.series.Series\n",
    "            The Series holding the training data associated with the\n",
    "            independent variable.\n",
    "        sr_X_test: pandas.core.series.Series\n",
    "            The Series holding the test data associated with the\n",
    "            independent variable.\n",
    "        df_Y_train: pandas.core.frame.DataFrame\n",
    "            The DataFrame holding the training data associated with the\n",
    "            dependent variable.\n",
    "        df_Y_train: pandas.core.frame.DataFrame\n",
    "            The DataFrame holding the test data associated with the\n",
    "            dependent variable.\n",
    "        K: int\n",
    "            Number of knots used to fit the natural cubic spline.\n",
    "    Returns:\n",
    "        tuple: (statsmodels.genmod.generalized_linear_model.GLM.fit, MSE)\n",
    "            The first element of the returned tuple is the fitted model\n",
    "            on the training data. The second element respresents the test\n",
    "            MSE.\n",
    "    \"\"\"\n",
    "    \n",
    "    # https://github.com/pydata/patsy/issues/108\n",
    "    # https://www.statsmodels.org/dev/generated/statsmodels.gam.smooth_basis.BSplines.html\n",
    "    # cubic spline with K knots uses has K+4 degrees of freedom\n",
    "    # so number of basis functions df = K + 3\n",
    "    df = K + 3\n",
    "    df_X_transformed_train = dmatrix(f'cr(sr_X_train, df={df})',\n",
    "                                     {'sr_X_train': sr_X_train}, return_type='dataframe')\n",
    "\n",
    "    assert df_X_transformed_train.shape[1] == df + 1\n",
    "\n",
    "    # Build a regular linear model from the splines\n",
    "    fitted_model = sm.GLM(df_Y_train, df_X_transformed_train).fit()\n",
    "\n",
    "    sr_Y_pred = fitted_model.predict(dmatrix(f'cr(sr_X_test, df={df})', \n",
    "                                       {'sr_X_test': sr_X_test}, return_type='dataframe'))\n",
    "\n",
    "    MSE = mean_squared_error(df_Y_test, sr_Y_pred)\n",
    "    \n",
    "    return fitted_model, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T20:01:17.420978Z",
     "start_time": "2020-02-09T20:01:17.325639Z"
    }
   },
   "outputs": [],
   "source": [
    "def plotCubicSpines(df_X, df_Y, dict_models):\n",
    "    \"\"\"\n",
    "    This function plots cubic splines curves with various \n",
    "    number of knots all fit on the same data df_X and df_Y.\n",
    "    \n",
    "    Args:\n",
    "        df_X: pandas.core.frame.DataFrame\n",
    "            The DataFrame holding the dependent variable.\n",
    "        df_Y: pandas.core.frame.DataFrame\n",
    "            The DataFrame holding the dependent variable.\n",
    "        dict_models: dict\n",
    "            Dictionary that maps the number of knots as \n",
    "            keys with values as the fitted model of the type\n",
    "            statsmodels.genmod.generalized_linear_model.GLM.fit.\n",
    "    \"\"\"\n",
    "    independent = df_X.columns[0]\n",
    "    dependent = df_Y.columns[0]\n",
    "    X_pred = np.linspace(df_X[independent].min(), df_X[independent].max(), num=df_X[independent].shape[0], endpoint=True)\n",
    "    \n",
    "\n",
    "    fig, ax1 = plt_func.subplots(1, 1, constrained_layout=True, figsize=(8, 4))\n",
    "    _ = ax1.scatter(df_X[independent], df_Y, s=1)\n",
    "    for knots, fitted_model in dict_models.items():\n",
    "        # https://github.com/pydata/patsy/issues/108\n",
    "        # https://www.statsmodels.org/dev/generated/statsmodels.gam.smooth_basis.BSplines.html\n",
    "        # cubic spline with K knots uses has K+4 degrees of freedom\n",
    "        # so number of basis functions df = K + 3\n",
    "        df = knots + 3\n",
    "        sr_Y_pred = fitted_model.predict(dmatrix(f'bs(X_pred, df={df}, include_intercept=True)',\n",
    "                                     {'X_pred': X_pred}, return_type='dataframe'))\n",
    "        _ = ax1.plot(X_pred, sr_Y_pred, label=f'{knots} knots')\n",
    "    _ = ax1.legend()\n",
    "    _ = ax1.set_xlabel(independent)\n",
    "    _ = ax1.set_ylabel(dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T20:01:17.510508Z",
     "start_time": "2020-02-09T20:01:17.426648Z"
    }
   },
   "outputs": [],
   "source": [
    "def plotNaturalCubicSpines(df_X, df_Y, dict_models):\n",
    "    \"\"\"\n",
    "    This function plots natural cubic splines curves with \n",
    "    various number of knots all fit on the same data df_X \n",
    "    and df_Y.\n",
    "    \n",
    "    Args:\n",
    "        df_X: pandas.core.frame.DataFrame\n",
    "            The DataFrame holding the dependent variable.\n",
    "        df_Y: pandas.core.frame.DataFrame\n",
    "            The DataFrame holding the dependent variable.\n",
    "        dict_models: dict\n",
    "            Dictionary that maps the number of knots as \n",
    "            keys with values as the fitted model of the type\n",
    "            statsmodels.genmod.generalized_linear_model.GLM.fit.\n",
    "    \"\"\"\n",
    "    independent = df_X.columns[0]\n",
    "    dependent = df_Y.columns[0]\n",
    "    X_pred = np.linspace(df_X[independent].min(), df_X[independent].max(), num=df_X[independent].shape[0], endpoint=True)\n",
    "    \n",
    "\n",
    "    fig, ax1 = plt_func.subplots(1, 1, constrained_layout=True, figsize=(8, 4))\n",
    "    _ = ax1.scatter(df_X[independent], df_Y, s=1)\n",
    "    for knots, fitted_model in dict_models.items():\n",
    "        # https://github.com/pydata/patsy/issues/108\n",
    "        # https://www.statsmodels.org/dev/generated/statsmodels.gam.smooth_basis.BSplines.html\n",
    "        # cubic spline with K knots uses has K+4 degrees of freedom\n",
    "        # so number of basis functions df = K + 3\n",
    "        df = knots + 3\n",
    "        sr_Y_pred = fitted_model.predict(dmatrix(f'cr(X_pred, df={df})',\n",
    "                                     {'X_pred': X_pred}, return_type='dataframe'))\n",
    "        _ = ax1.plot(X_pred, sr_Y_pred, label=f'{knots} knots')\n",
    "    _ = ax1.legend()\n",
    "    _ = ax1.set_xlabel(independent)\n",
    "    _ = ax1.set_ylabel(dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T20:01:17.690593Z",
     "start_time": "2020-02-09T20:01:17.512984Z"
    }
   },
   "outputs": [],
   "source": [
    "def plotWrapperCubicSplines(df_X, df_X_train, df_X_test, df_Y, df_Y_train, df_Y_test, natural):\n",
    "    \"\"\"\n",
    "    This function is a wrapper that plots the test MSE vs \n",
    "    the number of knots as a result of fitting (natural) \n",
    "    cubic splines.\n",
    "    \n",
    "    Args:\n",
    "        df_X: pandas.core.frame.DataFrame\n",
    "            The DataFrame holding the dependent variable.\n",
    "        df_X_train: pandas.core.frame.DataFrame\n",
    "            The DataFrame holding the training data associated with the\n",
    "            independent variable.\n",
    "        df_X_test: pandas.core.frame.DataFrame\n",
    "            The DataFrame holding the test data associated with the\n",
    "            independent variable.\n",
    "        df_Y: pandas.core.frame.DataFrame\n",
    "            The DataFrame holding the dependent variable.\n",
    "        df_Y_train: pandas.core.frame.DataFrame\n",
    "            The DataFrame holding the training data associated with the\n",
    "            dependent variable.\n",
    "        df_Y_test: pandas.core.frame.DataFrame\n",
    "            The DataFrame holding the test data associated with the\n",
    "            dependent variable.\n",
    "        natural: bool\n",
    "            Boolean variable that decide to fit cubic splines (False)\n",
    "            or natural cubic splines (True).\n",
    "    \"\"\"\n",
    "    independent = df_X.columns[0]\n",
    "    dict_models = {knots: None for knots in range(1, 10)}\n",
    "    results = {}\n",
    "    for knots in dict_models:\n",
    "        if natural:\n",
    "            fitted, MSE = fitNaturalCubicSplines(df_X_train[independent], df_X_test[independent], \n",
    "                        df_Y_train, df_Y_test, K=knots)\n",
    "        else:\n",
    "            fitted, MSE = fitCubicSplines(df_X_train[independent], df_X_test[independent], \n",
    "                            df_Y_train, df_Y_test, K=knots)\n",
    "\n",
    "        dict_models[knots] = fitted\n",
    "        results[knots] = MSE\n",
    "        \n",
    "    knots_lst = list(results.keys())\n",
    "    MSE_lst = list(results.values())\n",
    "    std_dev = np.std(MSE_lst)\n",
    "    min_mse = min(results, key=results.get)\n",
    "    min_val = results[min_mse]\n",
    "    knots_lst.remove(min_mse)  # we plot these 'best results' with a different marker\n",
    "    MSE_lst.remove(min_val)  # we plot these 'best results' with a different marker\n",
    "\n",
    "    fig, ax1 = plt_func.subplots(1, 1, constrained_layout=True, figsize=(8, 4))\n",
    "    _ = ax1.scatter(knots_lst, MSE_lst)\n",
    "    _ = ax1.scatter(min_mse, min_val, marker='x', s=40, c=plt_func.rcParams['axes.prop_cycle'].by_key()['color'][0], label='smallest MSE')\n",
    "    min_val_2std = min_val - 0.2*std_dev\n",
    "    _ = ax1.axhline(y=min_val_2std, linestyle='dashed', linewidth=0.5, c=plt_func.rcParams['axes.prop_cycle'].by_key()['color'][1], label='0.2 standard deviation')\n",
    "    _ = ax1.axhline(y=min_val + 0.2*std_dev, linestyle='dashed', linewidth=0.5, c=plt_func.rcParams['axes.prop_cycle'].by_key()['color'][1])\n",
    "    _ = ax1.set_xlabel(r'number of knots')\n",
    "    _ = ax1.set_ylabel('Cross-validation MSE')\n",
    "    _ = ax1.legend()\n",
    "    \n",
    "    diff = max(MSE_lst) - min_val_2std\n",
    "    if diff > 70:\n",
    "        _ = ax1.set_ylim(ymin = min_val_2std - (min_val_2std % 10) - 10)\n",
    "    elif diff > 40:\n",
    "        _ = ax1.set_ylim(ymin = min_val_2std - (min_val_2std % 10))\n",
    "    else:\n",
    "        _ = ax1.set_ylim(ymin = min_val_2std - (min_val_2std % 1) - 1)\n",
    "\n",
    "    if natural:\n",
    "        plotNaturalCubicSpines(df_X[[independent]], df_Y, dict_models)\n",
    "    else:\n",
    "        plotCubicSpines(df_X[[independent]], df_Y, dict_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
